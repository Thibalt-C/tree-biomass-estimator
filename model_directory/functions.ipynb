{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bfab809-398e-4b94-9c4e-4864ef483663",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Functions scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "312f95df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intensity_filter(file,slices,ROI):\n",
    "    \n",
    "    #libraries import\n",
    "    import laspy\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt #removable if make_plots!='y'\n",
    "    import time #removable\n",
    "    \n",
    "    start = time.time() #removable\n",
    "\n",
    "    #variable type test\n",
    "    if np.array([type(file)!=str , np.array([type(slices)!=int,type(slices)!=float]).all()]).any() :\n",
    "        return print(\"error: wrong type of variable\")\n",
    "    \n",
    "    #will perform if ROI is set to 'auto'\n",
    "    def autoROI(file):\n",
    "        import laspy\n",
    "        import numpy as np\n",
    "        path = '1-your_initial_data/' \n",
    "        cloud = laspy.read(path+file)\n",
    "        scale = cloud.header.scales\n",
    "        x , y , z = cloud.X*scale[0] , cloud.Y*scale[1] , cloud.Z*scale[2]\n",
    "        x , y , z = x.reshape(len(x),1) , y.reshape(len(y),1) , z.reshape(len(z),1)\n",
    "        intensity = cloud.intensity.reshape(len(x),1)\n",
    "        CLOUD = np.hstack([x,y,z,intensity])\n",
    "        mean , std = CLOUD[:,:2].mean(axis=0) , CLOUD[:,:2].std(axis=0)\n",
    "        ROIxmin , ROIymin = mean-2*std \n",
    "        ROIxmax , ROIymax = mean+2*std\n",
    "        return [ROIxmin,ROIxmax,ROIymin,ROIymax]\n",
    "    \n",
    "    if type(ROI)!=list:\n",
    "        if ROI == 'auto':\n",
    "            ROI = autoROI(file)\n",
    "        else:\n",
    "            return print(\"error: wrong type of variable\")\n",
    "    \n",
    "    if len(ROI)!=4:\n",
    "        return print('ROI should be a list of 4 limits: [xmin,xmax,ymin,ymax]. Please try again.')\n",
    "    \n",
    "    \n",
    "    #importing the cloud\n",
    "    path = '1-your_initial_data/' \n",
    "    path1 = path + file\n",
    "    \n",
    "    cloud = laspy.read(path1)\n",
    "    scale = cloud.header.scales\n",
    "    x , y , z = cloud.X*scale[0] , cloud.Y*scale[1] , cloud.Z*scale[2]\n",
    "    x , y , z = x.reshape(len(x),1) , y.reshape(len(y),1) , z.reshape(len(z),1)\n",
    "    intensity = cloud.intensity.reshape(len(x),1)\n",
    "    CLOUD = np.hstack([x,y,z,intensity])\n",
    "    n , n_param = CLOUD.shape\n",
    "    print('')\n",
    "    print(file,':',n,'points and',n_param,'parameters found')\n",
    "    \n",
    "    #plotting if variable make_plots is active\n",
    "    if make_plots=='y': \n",
    "        def scatter_hist(x, y, ax, ax_histx, ax_histy, c): #source : matplotlib documentation\n",
    "            ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "            ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "            ax.scatter(x, y,s=0.05,alpha=0.1,c=c)\n",
    "            binwidth = 2\n",
    "            xymax = max(np.max(np.abs(x)), np.max(np.abs(y)))\n",
    "            lim = (int(xymax/binwidth) + 1) * binwidth\n",
    "            bins = np.arange(-lim, lim + binwidth, binwidth)\n",
    "            ax_histx.hist(x, bins=bins,color=c)\n",
    "            ax_histy.hist(y, bins=bins, orientation='horizontal',color=c)\n",
    "\n",
    "        fig = plt.figure(figsize=(6,6))\n",
    "        gs = fig.add_gridspec(2, 2, width_ratios=(4, 1), height_ratios=(1, 4),\n",
    "                              left=0.1, right=0.9, bottom=0.1, top=0.9,\n",
    "                              wspace=0.05, hspace=0.05)\n",
    "        ax = fig.add_subplot(gs[1, 0])\n",
    "        plt.xlabel('x axis')\n",
    "        plt.ylabel('y axis')\n",
    "        ax_x = fig.add_subplot(gs[0, 0],sharex=ax)\n",
    "        ax_y = fig.add_subplot(gs[1, 1],sharey=ax)\n",
    "        scatter_hist(x, y, ax, ax_x, ax_y,'g')\n",
    "\n",
    "\n",
    "    #defining the ROI in the (x,y) plan\n",
    "    ROIxmax = ROI[1]\n",
    "    ROIxmin = ROI[0]\n",
    "    ROIymax = ROI[3]\n",
    "    ROIymin = ROI[2]\n",
    "    center = (ROIxmax+ROIxmin)/2 , (ROIymax+ROIymin)/2\n",
    "\n",
    "    if make_plots=='y': \n",
    "        ax.scatter([ROIxmin,ROIxmin,ROIxmax,ROIxmax],[ROIymin,ROIymax,ROIymin,ROIymax],marker='+',s=200,c='k')\n",
    "        plt.show()\n",
    "    \n",
    "    #reducing the cloud to ROI dimensions\n",
    "    CLOUD = CLOUD[CLOUD[:,0]>=ROIxmin]\n",
    "    CLOUD = CLOUD[CLOUD[:,0]<=ROIxmax]\n",
    "    CLOUD = CLOUD[CLOUD[:,1]>=ROIymin]\n",
    "    CLOUD = CLOUD[CLOUD[:,1]<=ROIymax]\n",
    "    \n",
    "    #plotting if variable make_plots is active\n",
    "    if make_plots=='y': \n",
    "        plt.subplots(1,2,sharey=True)\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.scatter(CLOUD[:,0],CLOUD[:,1],s=0.05,alpha=0.01,c='g')\n",
    "        plt.xlabel('X axis')\n",
    "        plt.ylabel('Y axis')\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.scatter(CLOUD[:,2],CLOUD[:,1],s=0.05,alpha=0.01,c='g')\n",
    "        plt.xlabel('Z axis')\n",
    "        plt.show()\n",
    "    \n",
    "    print(np.round(100*len(CLOUD)/n,2),'% of points within the region of interest (ROI)')\n",
    "    \n",
    "    #for this slicing we will plot histograms showing the repartition of the points following their intensity\n",
    "    slices = np.linspace(CLOUD[:,2].min(),CLOUD[:,2].max(),slices+1)\n",
    "    F_CLOUD = np.zeros((0,4))\n",
    "    for i in range ( slices ) :\n",
    "        low , high = slices[i] , slices[i+1]\n",
    "        slc = CLOUD[CLOUD[:,2]>=low]\n",
    "        slc = slc[slc[:,2]<high]\n",
    "        if len(slc)>0:\n",
    "            initial = len(slc)\n",
    "            slc = slc[slc[:,3]>=slc[:,3].mean()+2*slc[:,3].std()]\n",
    "            final = len(slc)\n",
    "            F_CLOUD = np.vstack([F_CLOUD,slc])\n",
    "            print('slice',i+1,np.round(100*final/initial,2),'% of points remaining')\n",
    "\n",
    "    print('Remaining points',len(F_CLOUD),f'({np.round(len(F_CLOUD)*100/n,2)} % of the initial file)')\n",
    "    \n",
    "    #plotting if variable make_plots is active\n",
    "    if make_plots=='y': \n",
    "        plt.subplots(2,1,sharex=True,figsize=(5,10))\n",
    "        plt.subplot(2,1,1)\n",
    "        plt.title(f'{slices} slices')\n",
    "        plt.scatter(F_CLOUD[:,0],F_CLOUD[:,2],s=0.1,alpha=0.01,c='g')\n",
    "        plt.ylabel('Z axis')\n",
    "        plt.subplot(2,1,2)\n",
    "        plt.scatter(F_CLOUD[:,0],F_CLOUD[:,1],s=0.05,alpha=0.01,c='g')\n",
    "        plt.ylabel('Y axis')\n",
    "        plt.xlabel('X axis')\n",
    "        plt.show()\n",
    "    \n",
    "    #export\n",
    "    path2 = '2-intensity_filtered_point_cloud/'\n",
    "    output_file = laspy.LasData(laspy.LasHeader(version=\"1.2\"))\n",
    "    output_file.X = F_CLOUD[:,0]/scale[0]\n",
    "    output_file.Y = F_CLOUD[:,1]/scale[1]\n",
    "    output_file.Z = F_CLOUD[:,2]/scale[2]\n",
    "    output_file.x = F_CLOUD[:,0]\n",
    "    output_file.y = F_CLOUD[:,1]\n",
    "    output_file.z = F_CLOUD[:,2]\n",
    "    output_file.intensity = F_CLOUD[:,3]\n",
    "    output_file.write(path2+f\"{file[:-4]}_ROI.las\")\n",
    "    \n",
    "    time = time.time() - start #removable\n",
    "    \n",
    "    return print(f'done，processed in {np.round(time,2)} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d6190d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DBSCAN_clustering(file,ROIzmin,ROIzmax,eps,min_samples):\n",
    "    \n",
    "    #libraries import\n",
    "    import laspy\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt #removable if make_plots!='y'\n",
    "    import os , os.path, shutil #removable\n",
    "    import time #removable\n",
    "    \n",
    "    start = time.time() #removable\n",
    "    \n",
    "    #path to the classified point cloud of the previous step\n",
    "    path = '3-trees_point_cloud/trees/'\n",
    "    path1 = path + file\n",
    "    #importing the cloud\n",
    "    cloud = laspy.read(path1)\n",
    "    scale = cloud.header.scales\n",
    "    x , y , z = cloud.X*scale[0] , cloud.Y*scale[1] , cloud.Z*scale[2]\n",
    "    x , y , z = x.reshape(len(x),1) , y.reshape(len(y),1) , z.reshape(len(z),1)\n",
    "    intensity = cloud.intensity.reshape(len(x),1)\n",
    "    CLOUD = np.hstack([x,y,z,intensity]) #our cloud is now a n by 4 matrix (n = number of points)\n",
    "    \n",
    "    #DBSCAN will only operate on a thin slice so we save the rest on an other variable\n",
    "    ROI = CLOUD\n",
    "    #slice we will focus on\n",
    "    CLOUD = CLOUD[CLOUD[:,2]<=ROIzmax]\n",
    "    CLOUD = CLOUD[CLOUD[:,2]>=ROIzmin]\n",
    "    #ROI parts under and above the slice\n",
    "    UP_CLOUD = ROI[ROI[:,2]>ROIzmax]\n",
    "    DOWN_CLOUD = ROI[ROI[:,2]<ROIzmin]\n",
    "    \n",
    "    #we compute DBSCAN with the input parameters\n",
    "    db = DBSCAN(eps=eps,min_samples=min_samples).fit(CLOUD)\n",
    "    #we get the labels corresponding to clusters IDs\n",
    "    labels = db.labels_\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise if present\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "\n",
    "    print(n_clusters_,'clusters detected,' , n_noise_,f'points of noise ({np.round(100*n_noise_/len(CLOUD))} %)')\n",
    "\n",
    "    #saving trunks clusters in a matrix with a fifth column to identify each one\n",
    "    clusters = np.hstack([CLOUD[labels!=-1],labels[labels!=-1].reshape(len(labels[labels!=-1]),1)])\n",
    "    \n",
    "    #getting the centroid of each cluster\n",
    "    centroids = np.zeros((0,3))\n",
    "    for cluster in set(clusters[:,4]):\n",
    "        cl = clusters[clusters[:,4]==cluster][:,:3]\n",
    "        centroid = cl.mean(axis=0)\n",
    "        centroids = np.vstack([centroids,centroid])\n",
    "    \n",
    "    #plotting if variable make_plots is active\n",
    "    if make_plots=='y':\n",
    "        plt.figure(figsize=(7,7))    \n",
    "        i=0\n",
    "        for cluster in set(clusters[:,4]):\n",
    "            i+=1\n",
    "            points = clusters[clusters[:,4]==cluster][:,:3]\n",
    "            xmin , xmax = points[:,0].min() , points[:,0].max()\n",
    "            ymin , ymax = points[:,1].min() , points[:,1].max()\n",
    "            centroid = centroids[i-1]\n",
    "            plt.scatter(points[:,0],points[:,1],s=2,alpha=0.5,c='y')\n",
    "            plt.plot(np.linspace(xmin,xmax,2),\n",
    "                     np.array([centroid[1],centroid[1]]),\n",
    "                     c='k',lw=1)\n",
    "            plt.plot(np.array([centroid[0],centroid[0]]),\n",
    "                     np.linspace(ymin,ymax,2),\n",
    "                     c='k',lw=1)\n",
    "            plt.xlabel('X axis')\n",
    "            plt.ylabel('Y axis')\n",
    "        plt.show()\n",
    "    \n",
    "    #export\n",
    "    path_trunk = '4-separated_trees/DBSCAN_trunks/'\n",
    "    for root, dirs, files in os.walk(path_trunk): #removable\n",
    "        for f in files:                           #\n",
    "            os.unlink(os.path.join(root, f))      #\n",
    "        for d in dirs:                            #\n",
    "            shutil.rmtree(os.path.join(root, d))  #\n",
    "    i=0\n",
    "    for cluster in set(clusters[:,4]):\n",
    "        i = i+1\n",
    "        cluster_points = clusters[clusters[:,4]==cluster]\n",
    "        output_file = laspy.LasData(laspy.LasHeader(version=\"1.2\"))\n",
    "        output_file.x = cluster_points[:,0]\n",
    "        output_file.y = cluster_points[:,1]\n",
    "        output_file.z = cluster_points[:,2]\n",
    "        output_file.intensity = cluster_points[:,3]\n",
    "        output_file.write(f\"{path_trunk}trunk{i}.las\")\n",
    "        if i<10:\n",
    "            output_file.write(f\"{path_trunk}trunk0{i}.las\")\n",
    "        else:\n",
    "            output_file.write(f\"{path_trunk}trunk{i}.las\")\n",
    "    \n",
    "    t = time.time() - start\n",
    "    \n",
    "    print(f'done，processed in {t} sec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a8983e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging(dist_to_merge,ROIzmin,ROIzmax):\n",
    "    \n",
    "    import laspy\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt #removable if make_plots!='y'\n",
    "    import os , os.path, shutil #removable but has to be replaced by something else to not break the script\n",
    "    import time #removable\n",
    "    \n",
    "    start = time.time() #removable\n",
    "    \n",
    "    #DBSCAN clusters recovering\n",
    "    path_trunk = '4-separated_trees/DBSCAN_trunks/'\n",
    "    files = os.listdir(path_trunk)\n",
    "    files = np.array(files)\n",
    "    files = files[files!='.DS_Store']\n",
    "    files_names = []\n",
    "    for file in files:\n",
    "        files_names.append(file[:-4])\n",
    "    clusters = {}\n",
    "    i = 0\n",
    "    for file_name in files_names:\n",
    "        i += 1\n",
    "        file = files[i-1]\n",
    "        cloud = laspy.read(path_trunk+file)\n",
    "        scale = cloud.header.scales\n",
    "        x , y , z = cloud.X*scale[0] , cloud.Y*scale[1] , cloud.Z*scale[2]\n",
    "        x , y , z = x.reshape(len(x),1) , y.reshape(len(y),1) , z.reshape(len(z),1)\n",
    "        intensity = cloud.intensity.reshape(len(x),1)\n",
    "        clusters[f'{file_name}'] = np.hstack([x,y,z,intensity])\n",
    "        centroid = clusters[f'{file_name}'][:,:3].mean(axis=0)\n",
    "        clusters[f'{file_name}'] = clusters[f'{file_name}'] , centroid\n",
    "    \n",
    "    #3D Forest regions acquisition\n",
    "    path_3df_regions = '4-separated_trees/3DForest_segmented_regions/'\n",
    "    files = os.listdir(path_3df_regions)\n",
    "    files = np.array(files)\n",
    "    files = files[files!='.DS_Store']\n",
    "    files_names = []\n",
    "    for file in files:\n",
    "        files_names.append(file[:-4])\n",
    "    regions = {}\n",
    "    i = 0\n",
    "    for file_name in files_names:\n",
    "        i += 1\n",
    "        file = files[i-1]\n",
    "        regions[f'{file_name}'] = pd.read_csv(path_3df_regions+file,\n",
    "                                              encoding = \"utf-8\",\n",
    "                                              delimiter=' ',\n",
    "                                              header=None,\n",
    "                                              names=['x','y','z','intensity'])\n",
    "        regions[f'{file_name}'] = np.array(regions[f'{file_name}'])\n",
    "    for file_name in files_names:\n",
    "        Slice = regions[f'{file_name}'][regions[f'{file_name}'][:,2]<=ROIzmax]\n",
    "        Slice = Slice[Slice[:,2]>=ROIzmin]\n",
    "        if len(Slice)>0:\n",
    "            centroid = Slice[:,:3].mean(axis=0)\n",
    "        else:\n",
    "            centroid = np.array([np.nan,np.nan,np.nan])\n",
    "        regions[f'{file_name}'] = regions[f'{file_name}'] , centroid\n",
    "    \n",
    "    #merging\n",
    "    centroids = []\n",
    "    reg = []\n",
    "    j=0\n",
    "    for i in range (len(regions)):\n",
    "        key = list(regions.keys())[j]\n",
    "        centroid = regions[f'{key}'][1]\n",
    "        if np.isnan(centroid).any():\n",
    "            del (regions[f'{key}'])\n",
    "        else:\n",
    "            centroids.append(centroid)\n",
    "            reg.append(regions[f'{key}'][0])\n",
    "            j+=1\n",
    "    centroids = np.array(centroids)\n",
    "    centroids = centroids[~np.isnan(centroids).any(axis=1)]\n",
    "    \n",
    "    new_regions = []\n",
    "    count = 0\n",
    "    control = 'not_ok'\n",
    "    \n",
    "    if len(clusters)<=len(regions):\n",
    "        \n",
    "        for cluster in clusters.keys():\n",
    "            centroid = clusters[f'{cluster}'][1]\n",
    "            dist =  np.linalg.norm(centroids[:,:2]-centroid[:2],axis=1).min()\n",
    "            while dist<=dist_to_merge:\n",
    "                count += 1\n",
    "                control = 'ok'\n",
    "                idx_min = np.linalg.norm(centroids[:,:2]-centroid[:2],axis=1).argmin()\n",
    "                new_region = clusters[f'{cluster}'][0]\n",
    "                new_region = np.vstack([new_region,np.array(reg[idx_min])])\n",
    "                centroids = np.delete(centroids,idx_min,axis=0)\n",
    "                del(reg[idx_min])\n",
    "                dist =  np.linalg.norm(centroids[:,:2]-centroid[:2],axis=1).min()\n",
    "            if control=='ok':\n",
    "                if not any(np.array_equal(new_region, reg) for reg in new_regions):\n",
    "                    new_regions.append(new_region)\n",
    "    \n",
    "    else:\n",
    "        i=0\n",
    "        centr = []\n",
    "        regs = []\n",
    "        for cluster in clusters.keys():\n",
    "            r = clusters[f'{cluster}'][0]\n",
    "            centroid = clusters[f'{cluster}'][1]\n",
    "            centr.append(centroid)\n",
    "            regs.append(r)\n",
    "        centr = np.array(centr)\n",
    "        for region in reg:\n",
    "            i+=1\n",
    "            centroid = centroids[i-1]\n",
    "            dist =  np.linalg.norm(centr[:,:2]-centroid[:2],axis=1).min()\n",
    "            while dist<=dist_to_merge:\n",
    "                count += 1\n",
    "                control = 'ok'\n",
    "                idx_min = np.linalg.norm(centr[:,:2]-centroid[:2],axis=1).argmin()\n",
    "                new_region = np.vstack([region,np.array(regs[idx_min])])\n",
    "                centr = np.delete(centr,idx_min,axis=0)\n",
    "                del(regs[idx_min])\n",
    "                dist =  np.linalg.norm(centr[:,:2]-centroid[:2],axis=1).min()\n",
    "            if control=='ok':\n",
    "                if not any(np.array_equal(new_region, reg) for reg in new_regions):\n",
    "                    new_regions.append(new_region)\n",
    "\n",
    "    print(f'{len(new_regions)} regions detected after co-registration')\n",
    "    print (count)\n",
    "\n",
    "    #export\n",
    "    path_co_reg = '4-separated_trees/Co-registered_regions/'\n",
    "    for root, dirs, files in os.walk(path_co_reg):\n",
    "        for f in files:\n",
    "            os.unlink(os.path.join(root, f))\n",
    "        for d in dirs:\n",
    "            shutil.rmtree(os.path.join(root, d))\n",
    "    i=0\n",
    "    for region in new_regions:\n",
    "        i = i+1\n",
    "        output_file = laspy.LasData(laspy.LasHeader(version=\"1.2\"))\n",
    "        output_file.X = region[:,0]/scale[0]\n",
    "        output_file.Y = region[:,1]/scale[1]\n",
    "        output_file.Z = region[:,2]/scale[2]\n",
    "        output_file.x = region[:,0]\n",
    "        output_file.y = region[:,1]\n",
    "        output_file.z = region[:,2]\n",
    "        output_file.intensity = region[:,3]\n",
    "        if i<10:\n",
    "            output_file.write(f\"{path_co_reg}region0{i}.las\")\n",
    "        else:\n",
    "            output_file.write(f\"{path_co_reg}region{i}.las\")\n",
    "        \n",
    "    #plotting if variable make_plots is active    \n",
    "    if make_plots=='y':\n",
    "        \n",
    "        plt.figure(figsize=(8,8))\n",
    "\n",
    "        i=0\n",
    "\n",
    "        new_centroids = []\n",
    "        for region in new_regions:\n",
    "            i += 1\n",
    "            Slice = region[region[:,2]>=ROIzmin]\n",
    "            Slice = Slice[Slice[:,2]<=ROIzmax]\n",
    "            centroid = Slice[:,:3].mean(axis=0)\n",
    "            if i==1:\n",
    "                plt.scatter(centroid[0],centroid[1],marker='o',\n",
    "                            s=100,c='w',edgecolors='r',\n",
    "                            label='merged regions')\n",
    "            else:\n",
    "                plt.scatter(centroid[0],centroid[1],marker='o',\n",
    "                            s=100,c='w',edgecolors='r')\n",
    "        i=0\n",
    "\n",
    "        for cluster in clusters.keys() :\n",
    "            i += 1\n",
    "            points = clusters[f'{cluster}'][0]\n",
    "            xmin = points[:,0].max()\n",
    "            xmax = points[:,0].min()\n",
    "            centroid = clusters[f'{cluster}'][1]\n",
    "            if i==1:\n",
    "                plt.scatter(centroid[0],centroid[1],c='b',marker='+',label='DBSCAN clusters')\n",
    "            else:\n",
    "                plt.scatter(centroid[0],centroid[1],c='b',marker='+')\n",
    "\n",
    "        i=0\n",
    "\n",
    "        for region in regions.keys():\n",
    "            i += 1\n",
    "            points = regions[f'{region}'][0]\n",
    "            xmin = points[:,0].max()\n",
    "            xmax = points[:,0].min()\n",
    "            centroid = regions[f'{region}'][1]\n",
    "            if i==1:\n",
    "                plt.scatter(centroid[0],centroid[1],c='g',marker='x',label='3D Forest segmented regions') \n",
    "            else:\n",
    "                plt.scatter(centroid[0],centroid[1],c='g',marker='x')\n",
    "\n",
    "        plt.xlabel('X axis')\n",
    "        plt.ylabel('Y axis')   \n",
    "        plt.legend(fontsize='xx-small')\n",
    "        plt.show()\n",
    "        \n",
    "        if len(new_regions)<6:\n",
    "            if len(new_regions)/2 == int(len(new_regions)/2):\n",
    "                b = int(len(new_regions)/2)\n",
    "                a = int(len(new_regions)/b)\n",
    "        else:\n",
    "            b = 6\n",
    "            a = int(len(new_regions)/b)\n",
    "        \n",
    "        if a*b < len(new_regions):\n",
    "            a += 1\n",
    "        \n",
    "        plt.subplots(a,b,subplot_kw={\"projection\": \"3d\"},figsize=(b*3,a*3))\n",
    "        i=0\n",
    "        for region in new_regions:\n",
    "            i+=1\n",
    "            ax = plt.subplot(a,b,i,projection='3d')\n",
    "            ax.scatter(region[:,0],region[:,1],region[:,2],s=0.3,c='k',alpha=0.5)\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    t = time.time() - start #removable\n",
    "    \n",
    "    print(f'done，processed in {t} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16361ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_growing(ROI_file,max_distance_higher_points,max_distance_lower_points,ROIzmin,ROIzmax):\n",
    "    \n",
    "    #libraries import\n",
    "    import laspy\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt #removable if make_plots!='y'\n",
    "    import os , os.path, shutil #removable but has to be replaced by something else to not break the script\n",
    "    import time #removable\n",
    "    \n",
    "    start = time.time() #removable\n",
    "    \n",
    "    path_co_reg = '4-separated_trees/Co-registered_regions/'\n",
    "    files = os.listdir(path_co_reg)\n",
    "    files = np.array(files)\n",
    "    files = files[files!='.DS_Store']\n",
    "    files_names = []\n",
    "    for file in files:\n",
    "        files_names.append(file[:-4])\n",
    "    \n",
    "    new_regions = []\n",
    "    i = 0\n",
    "    for file_name in files_names:\n",
    "        i += 1\n",
    "        file = files[i-1]\n",
    "        cloud = laspy.read(path_co_reg+file)\n",
    "        scale = cloud.header.scales\n",
    "        x , y , z = cloud.X*scale[0] , cloud.Y*scale[1] , cloud.Z*scale[2]\n",
    "        x , y , z = x.reshape(len(x),1) , y.reshape(len(y),1) , z.reshape(len(z),1)\n",
    "        intensity = cloud.intensity.reshape(len(x),1)\n",
    "        new_regions.append(np.hstack([x,y,z,intensity]))\n",
    "    \n",
    "    path = '3-trees_point_cloud/trees/'\n",
    "    path1 = path + ROI_file  \n",
    "    cloud = laspy.read(path1)\n",
    "    scale = cloud.header.scales\n",
    "    x , y , z = cloud.X*scale[0] , cloud.Y*scale[1] , cloud.Z*scale[2]\n",
    "    x , y , z = x.reshape(len(x),1) , y.reshape(len(y),1) , z.reshape(len(z),1)\n",
    "    intensity = cloud.intensity.reshape(len(x),1)\n",
    "    CLOUD = np.hstack([x,y,z,intensity])\n",
    "    ROI = CLOUD\n",
    "    #slice we will focus on\n",
    "    CLOUD = CLOUD[CLOUD[:,2]<=ROIzmax]\n",
    "    CLOUD = CLOUD[CLOUD[:,2]>=ROIzmin]\n",
    "    #ROI parts under and above the slice\n",
    "    UP_CLOUD = ROI[ROI[:,2]>ROIzmax]\n",
    "    DOWN_CLOUD = ROI[ROI[:,2]<ROIzmin]\n",
    "\n",
    "    print('processing higher parts of the point cloud')\n",
    "    \n",
    "    unexplored = UP_CLOUD\n",
    "\n",
    "    initial = len(unexplored)\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for region in new_regions: #we will process region by region\n",
    "\n",
    "        init = len(region)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "         #initial point to start the growing of each region\n",
    "        start_point = region[region[:,2].argmax()]\n",
    "        previous_point = np.zeros((3,1)) #for now we set it as a (0,0,0) array\n",
    "\n",
    "        #we look for the closest neighbor of the centroid\n",
    "        neighbor = np.linalg.norm(unexplored[:,:3]-start_point[:3],axis=1).argmin() \n",
    "        #note that the first iteration has no condition\n",
    "        new_regions[i-1] = np.vstack([new_regions[i-1],unexplored[neighbor,:4]])\n",
    "\n",
    "        #now we prepare to iterate upward by updating the unexplored list, start_point and previous_point\n",
    "        previous_point = start_point\n",
    "        start_point = unexplored[neighbor,:3]\n",
    "        unexplored = np.delete(unexplored,neighbor,axis=0)\n",
    "\n",
    "        dist=0 #to initialize the next loop\n",
    "\n",
    "        #now we set the upward iteration, happening while the following condition is respected:\n",
    "        while dist<max_distance_higher_points:\n",
    "\n",
    "            #same as in initialization\n",
    "            neighbor = np.linalg.norm(unexplored[:,:3]-start_point,axis=1).argmin()\n",
    "            #dist is computed on the new point, if not respecting the condition anymore the iteration will break\n",
    "            dist = np.linalg.norm(unexplored[:,:3]-start_point,axis=1).min()\n",
    "\n",
    "\n",
    "            new_regions[i-1] = np.vstack([new_regions[i-1],unexplored[neighbor,:4]])\n",
    "\n",
    "            #we prepare the next iteration\n",
    "            previous_point = start_point\n",
    "            start_point = unexplored[neighbor,:3]\n",
    "            unexplored = np.delete(unexplored,neighbor,axis=0)\n",
    "\n",
    "\n",
    "        retrn = -1 #this is the backing step for the backward iteration\n",
    "        dist = 0 #we reset the dist variable just to permit to initiate the next loop right beneath\n",
    "\n",
    "\n",
    "        while dist<max_distance_higher_points:\n",
    "\n",
    "            return_point = new_regions[i-1][retrn,:] #by doing so we go backward on our added points\n",
    "            previous_point =  new_regions[i-1][retrn-1,:3]\n",
    "\n",
    "            #same principle as above\n",
    "            neighbor = np.linalg.norm(unexplored[:,:3]-return_point[:3],axis=1).argmin()\n",
    "\n",
    "            #we call a new variable that will create upward iteration within this loop\n",
    "            dist1 = np.linalg.norm(unexplored[:,:3]-return_point[:3],axis=1).min()\n",
    "\n",
    "            #this reproduces the condition we initially set, and makes the algorithm going backward again\n",
    "            if dist1>=max_distance_higher_points:\n",
    "                retrn += - 1\n",
    "                if abs(retrn-10)>len(new_regions[i-1]): #which means we can't go back up further\n",
    "                    dist = dist1 #we set this to break the loop because there is necessary dist1>=distance\n",
    "\n",
    "            #though if the condition is respected, the iteration goes upward for this branch\n",
    "            else:\n",
    "                start_point = return_point[:3] #to initialize, since we moved backward and have not updated it\n",
    "\n",
    "                dist2 = 0 #to initialize the next loop\n",
    "\n",
    "                #this reproduces the condition we initially set, and makes the algorithm going backward again\n",
    "                n=0 #we will use this variable to adjust the close region computing the third parameter\n",
    "\n",
    "                while dist2<max_distance_higher_points:\n",
    "\n",
    "                    #same principle as above\n",
    "                    neighbor = np.linalg.norm(unexplored[:,:3]-start_point,axis=1).argmin()\n",
    "\n",
    "                    #again we call a new variable to makes forward iteration if less than distance threshold\n",
    "                    dist2 = np.linalg.norm(unexplored[:,:3]-start_point,axis=1).min()\n",
    "\n",
    "                    new_regions[i-1] = np.vstack([new_regions[i-1],unexplored[neighbor,:4]])\n",
    "                    #and then we can prepare the next iteration\n",
    "                    previous_point = start_point\n",
    "                    start_point = unexplored[neighbor,:3]\n",
    "                    unexplored = np.delete(unexplored,neighbor,axis=0)\n",
    "\n",
    "        fin=len(new_regions[i-1])\n",
    "\n",
    "        print (f'slice {i}/{len(new_regions)}, growed by',np.round(100*fin/init),'%')\n",
    "\n",
    "    final = len(unexplored)\n",
    "\n",
    "    print('higher parts of the point cloud',100*final/initial,'% of growth')\n",
    "\n",
    "    print('processing lower parts of the point cloud')\n",
    "    \n",
    "    unexplored = DOWN_CLOUD\n",
    "\n",
    "    initial = len(unexplored)\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for region in new_regions: #we will process region by region\n",
    "\n",
    "        init = len(region)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "         #initial point to start the growing of each region\n",
    "        start_point = region[region[:,2].argmax()]\n",
    "        previous_point = np.zeros((3,1)) #for now we set it as a (0,0,0) array\n",
    "\n",
    "        #we look for the closest neighbor of the centroid\n",
    "        neighbor = np.linalg.norm(unexplored[:,:3]-start_point[:3],axis=1).argmin() \n",
    "        #note that the first iteration has no condition\n",
    "        new_regions[i-1] = np.vstack([new_regions[i-1],unexplored[neighbor,:4]])\n",
    "\n",
    "        #now we prepare to iterate upward by updating the unexplored list, start_point and previous_point\n",
    "        previous_point = start_point\n",
    "        start_point = unexplored[neighbor,:3]\n",
    "        unexplored = np.delete(unexplored,neighbor,axis=0)\n",
    "\n",
    "        dist=0 #to initialize the next loop\n",
    "\n",
    "        #now we set the upward iteration, happening while the following condition is respected:\n",
    "        while dist<max_distance_lower_points:\n",
    "\n",
    "            #same as in initialization\n",
    "            neighbor = np.linalg.norm(unexplored[:,:3]-start_point,axis=1).argmin()\n",
    "            #dist is computed on the new point, if not respecting the condition anymore the iteration will break\n",
    "            dist = np.linalg.norm(unexplored[:,:3]-start_point,axis=1).min()\n",
    "\n",
    "\n",
    "            new_regions[i-1] = np.vstack([new_regions[i-1],unexplored[neighbor,:4]])\n",
    "\n",
    "            #we prepare the next iteration\n",
    "            previous_point = start_point\n",
    "            start_point = unexplored[neighbor,:3]\n",
    "            unexplored = np.delete(unexplored,neighbor,axis=0)\n",
    "\n",
    "\n",
    "        retrn = -1 #this is the backing step for the backward iteration\n",
    "        dist = 0 #we reset the dist variable just to permit to initiate the next loop right beneath\n",
    "\n",
    "\n",
    "        while dist<max_distance_lower_points:\n",
    "\n",
    "            return_point = new_regions[i-1][retrn,:] #by doing so we go backward on our added points\n",
    "            previous_point =  new_regions[i-1][retrn-1,:3]\n",
    "\n",
    "            #same principle as above\n",
    "            neighbor = np.linalg.norm(unexplored[:,:3]-return_point[:3],axis=1).argmin()\n",
    "\n",
    "            #we call a new variable that will create upward iteration within this loop\n",
    "            dist1 = np.linalg.norm(unexplored[:,:3]-return_point[:3],axis=1).min()\n",
    "\n",
    "            #this reproduces the condition we initially set, and makes the algorithm going backward again\n",
    "            if dist1>=max_distance_lower_points:\n",
    "                retrn += - 1\n",
    "                if abs(retrn-10)>len(new_regions[i-1]): #which means we can't go back up further\n",
    "                    dist = dist1 #we set this to break the loop because there is necessary dist1>=distance\n",
    "\n",
    "            #though if the condition is respected, the iteration goes upward for this branch\n",
    "            else:\n",
    "                start_point = return_point[:3] #to initialize, since we moved backward and have not updated it\n",
    "\n",
    "                dist2 = 0 #to initialize the next loop\n",
    "\n",
    "                #this reproduces the condition we initially set, and makes the algorithm going backward again\n",
    "                n=0 #we will use this variable to adjust the close region computing the third parameter\n",
    "\n",
    "                while dist2<max_distance_lower_points:\n",
    "\n",
    "                    #same principle as above\n",
    "                    neighbor = np.linalg.norm(unexplored[:,:3]-start_point,axis=1).argmin()\n",
    "\n",
    "                    #again we call a new variable to makes forward iteration if less than distance threshold\n",
    "                    dist2 = np.linalg.norm(unexplored[:,:3]-start_point,axis=1).min()\n",
    "\n",
    "                    new_regions[i-1] = np.vstack([new_regions[i-1],unexplored[neighbor,:4]])\n",
    "                    #and then we can prepare the next iteration\n",
    "                    previous_point = start_point\n",
    "                    start_point = unexplored[neighbor,:3]\n",
    "                    unexplored = np.delete(unexplored,neighbor,axis=0)\n",
    "\n",
    "        fin=len(new_regions[i-1])\n",
    "\n",
    "        print (f'slice {i}/{len(new_regions)}, growed by',np.round(100*fin/init),'%')\n",
    "        \n",
    "\n",
    "    final = len(unexplored)\n",
    "\n",
    "    print('lower parts of the point cloud',100*final/initial,'% of growth')\n",
    "    \n",
    "    path_RG = '4-separated_trees/After_region_growing/'\n",
    "\n",
    "    for root, dirs, files in os.walk(path_RG):\n",
    "        for f in files:\n",
    "            os.unlink(os.path.join(root, f))\n",
    "        for d in dirs:\n",
    "            shutil.rmtree(os.path.join(root, d))\n",
    "\n",
    "    i=0\n",
    "    for region in new_regions:\n",
    "\n",
    "        i = i+1\n",
    "        output_file = laspy.LasData(laspy.LasHeader(version=\"1.2\"))\n",
    "        output_file.X = region[:,0]/scale[0]\n",
    "        output_file.Y = region[:,1]/scale[1]\n",
    "        output_file.Z = region[:,2]/scale[2]\n",
    "        output_file.x = region[:,0]\n",
    "        output_file.y = region[:,1]\n",
    "        output_file.z = region[:,2]\n",
    "        output_file.intensity = region[:,3]\n",
    "        if i<10:\n",
    "            output_file.write(f\"{path_RG}reg0{i}.las\")\n",
    "        else:\n",
    "            output_file.write(f\"{path_RG}reg{i}.las\")\n",
    "    \n",
    "    \n",
    "    if make_plots=='y':\n",
    "        \n",
    "        if len(new_regions)<6:\n",
    "            if len(new_regions)/2 == int(len(new_regions)/2):\n",
    "                b = int(len(new_regions)/2)\n",
    "                a = int(len(new_regions)/b)\n",
    "        else:\n",
    "            b = 6\n",
    "            a = int(len(new_regions)/b)\n",
    "        \n",
    "        if a*b < len(new_regions):\n",
    "            a += 1\n",
    "    \n",
    "        plt.subplots(a,b,subplot_kw={\"projection\": \"3d\"},figsize=(b*3,a*3))\n",
    "        i=0\n",
    "        for region in new_regions:\n",
    "            i+=1\n",
    "            ax = plt.subplot(a,b,i,projection='3d')\n",
    "            ax.scatter(region[:,0],region[:,1],region[:,2],s=0.1,c='k',alpha=0.5)\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    t = time.time() - start #removable\n",
    "    \n",
    "    print(f'done，processed in {t} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1351491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_nbr_reinforcement(original_file, general_threshold, crown_threshold,height_for_crown_threshold,minimum_heigh):\n",
    "    \n",
    "    import laspy\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os , os.path, shutil\n",
    "    import time\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    path_RG = '4-separated_trees/After_region_growing/'\n",
    "    files = os.listdir(path_RG)\n",
    "    files = np.array(files)\n",
    "    files = files[files!='.DS_Store']\n",
    "    files_names = []\n",
    "    for file in files:\n",
    "        files_names.append(file[:-4])\n",
    "    \n",
    "    new_regions = []\n",
    "    i = 0\n",
    "    for file_name in files_names:\n",
    "        i += 1\n",
    "        file = files[i-1]\n",
    "        cloud = laspy.read(path_RG+file)\n",
    "        scale = cloud.header.scales\n",
    "        x , y , z = cloud.X*scale[0] , cloud.Y*scale[1] , cloud.Z*scale[2]\n",
    "        x , y , z = x.reshape(len(x),1) , y.reshape(len(y),1) , z.reshape(len(z),1)\n",
    "        intensity = cloud.intensity.reshape(len(x),1)\n",
    "        new_regions.append(np.hstack([x,y,z,intensity]))\n",
    "    \n",
    "    path_original_data = '1-your_initial_data/'\n",
    "\n",
    "    leaves = laspy.read(path_original_data+original_file)\n",
    "\n",
    "    x , y , z , intensity = leaves.x , leaves.y , leaves.z , leaves.intensity\n",
    "    x = np.array(leaves.x).reshape((len(x),1))\n",
    "    y = np.array(leaves.y).reshape((len(x),1))\n",
    "    z = np.array(leaves.z).reshape((len(x),1))\n",
    "    intensity = np.array(leaves.intensity).reshape((len(x),1))\n",
    "\n",
    "    dense_ROI = np.hstack([x , y , z , intensity])\n",
    "    \n",
    "    regions = np.zeros((0,5))\n",
    "    i=0\n",
    "    for region in new_regions:\n",
    "        i+=1\n",
    "        regions = np.vstack([regions,np.hstack([region,i*np.ones((len(region),1))])])\n",
    "    \n",
    "    unexplored = np.hstack([dense_ROI,np.zeros((len(dense_ROI),1))])\n",
    "    merged = np.vstack([unexplored,regions])\n",
    "    \n",
    "    knn = NearestNeighbors(n_neighbors=10).fit(merged[:,:3])\n",
    "    distance_mat, neighbours_mat = knn.kneighbors(merged[:,:3])\n",
    "    neighbours_mat[distance_mat>=general_threshold] = -9999\n",
    "    nbr = neighbours_mat[:,1:]\n",
    "    nbr = nbr[:-len(regions)]\n",
    "    \n",
    "    index = -1\n",
    "    initial = len(merged[merged[:,4]!=0])\n",
    "    for row in nbr:\n",
    "        index += 1\n",
    "        found = 'n'\n",
    "        while found == 'n':\n",
    "            for element in row:\n",
    "                if element !=-9999:\n",
    "                    if merged[element,4]!=0:\n",
    "                        merged[index,4]=merged[element,4]\n",
    "                        found = 'y'\n",
    "                else:\n",
    "                    found = 'y'\n",
    "                found = 'y'\n",
    "    final = len(merged[merged[:,4]!=0])\n",
    "    print('+',np.round(100*(final-initial)/initial),'%')\n",
    "    \n",
    "    restricted_merged = merged[merged[:,2]>=height_for_crown_threshold+1.0]\n",
    "    gradient_merged = merged[merged[:,2]>=height_for_crown_threshold]\n",
    "    gradient_merged = gradient_merged[gradient_merged[:,2]<height_for_crown_threshold+1.0]\n",
    "    h_regions = restricted_merged[restricted_merged[:,4]!=0]\n",
    "    regions = merged[merged[:,4]!=0]\n",
    "\n",
    "    nb_slices = 10\n",
    "    slices = np.linspace(height_for_crown_threshold,height_for_crown_threshold+1.0,nb_slices+1)\n",
    "    min_dist_grad = np.linspace(general_threshold,crown_threshold,nb_slices)\n",
    "    for i in range(nb_slices):\n",
    "        low , high = slices[i] , slices[i+1]\n",
    "        slice_merged = gradient_merged[gradient_merged[:,2]<high]\n",
    "        slice_merged = slice_merged[slice_merged[:,2]>=low]\n",
    "        knn = NearestNeighbors(n_neighbors=50).fit(slice_merged[:,:3])\n",
    "        distance_mat, neighbours_mat = knn.kneighbors(slice_merged[:,:3])\n",
    "        neighbours_mat[distance_mat>=min_dist_grad[i]] = -9999\n",
    "        nbr = neighbours_mat[:,1:]\n",
    "        slc_regions = slice_merged[slice_merged[:,4]!=0]\n",
    "        nbr = nbr[:-len(slc_regions)]\n",
    "        index = -1\n",
    "        initial = len(slice_merged[slice_merged[:,4]!=0])\n",
    "        for row in nbr:\n",
    "            index += 1\n",
    "            found = 'n'\n",
    "            while found == 'n':\n",
    "                for element in row:\n",
    "                    if element !=-9999:\n",
    "                        if slice_merged[element,4]!=0:\n",
    "                            slice_merged[index,4]=slice_merged[element,4]\n",
    "                            found = 'y'\n",
    "                    else:\n",
    "                        found = 'y'\n",
    "                    found = 'y'\n",
    "        final = len(slice_merged[slice_merged[:,4]!=0])\n",
    "        print(f'{i+1}/{nb_slices}: +',np.round(100*(final-initial)/initial),'%')\n",
    "        merged = np.vstack([merged,slice_merged])\n",
    "        merged = np.unique(merged,axis=0)\n",
    "\n",
    "    \n",
    "    \n",
    "    knn = NearestNeighbors(n_neighbors=50).fit(restricted_merged[:,:3])\n",
    "    distance_mat, neighbours_mat = knn.kneighbors(restricted_merged[:,:3])\n",
    "    neighbours_mat[distance_mat>=crown_threshold] = -9999\n",
    "    nbr = neighbours_mat[:,1:]\n",
    "    nbr = nbr[:-len(h_regions)]\n",
    "    \n",
    "    index = -1\n",
    "    initial = len(restricted_merged[restricted_merged[:,4]!=0])\n",
    "    for row in nbr:\n",
    "        index += 1\n",
    "        found = 'n'\n",
    "        while found == 'n':\n",
    "            for element in row:\n",
    "                if element !=-9999:\n",
    "                    if restricted_merged[element,4]!=0:\n",
    "                        restricted_merged[index,4]=restricted_merged[element,4]\n",
    "                        found = 'y'\n",
    "                else:\n",
    "                    found = 'y'\n",
    "                found = 'y'\n",
    "    final = len(restricted_merged[restricted_merged[:,4]!=0])\n",
    "    print('crown grown by: +',np.round(100*(final-initial)/initial),'%')\n",
    "    \n",
    "    merged = np.vstack([merged,restricted_merged])\n",
    "    merged = np.unique(merged,axis=0)\n",
    "\n",
    "    i=0\n",
    "    height = []\n",
    "    C = []\n",
    "    for region in list(set(merged[:,4]))[1:]:\n",
    "        i+=1\n",
    "        points = merged[merged[:,4]==region]\n",
    "        h = points[:,2].max()-points[:,2].min()\n",
    "        if h<minimum_heigh:\n",
    "            c= 'r'\n",
    "        else:\n",
    "            c = 'g'\n",
    "        C.append(c)\n",
    "    C=np.array(C)   \n",
    "    print(100*len(C[C=='g'])/len(C),'% of the trees passed the minimum height')\n",
    "\n",
    "    if make_plots=='y':\n",
    "        \n",
    "        if len(list(set(merged[:,4]))[1:])<6:\n",
    "            if len(list(set(merged[:,4]))[1:])/2 == int(len(list(set(merged[:,4]))[1:])/2):\n",
    "                b = int(len(list(set(merged[:,4]))[1:])/2)\n",
    "                a = int(len(list(set(merged[:,4]))[1:])/b)\n",
    "        else:\n",
    "            b = 6\n",
    "            a = int(len(list(set(merged[:,4]))[1:])/b)\n",
    "\n",
    "        if a*b < len(list(set(merged[:,4]))[1:]):\n",
    "            a += 1\n",
    "\n",
    "        plt.subplots(a,b,subplot_kw={\"projection\": \"3d\"},figsize=(b*3,a*3))\n",
    "        i=0\n",
    "        for region in list(set(merged[:,4]))[1:]:\n",
    "            i+=1\n",
    "            ax = plt.subplot(a,b,i,projection='3d')\n",
    "            points = merged[merged[:,4]==region]\n",
    "            ax.scatter(points[:,0],points[:,1],points[:,2],s=0.1,c=C[i-1],alpha=0.5)\n",
    "        plt.show()\n",
    "    \n",
    "    path_tree = '4-separated_trees/Trees/'\n",
    "    path_not_satisfying = '4-separated_trees/not_satisfying/'\n",
    "    for root, dirs, files in os.walk(path_tree):\n",
    "        for f in files:\n",
    "            os.unlink(os.path.join(root, f))\n",
    "        for d in dirs:\n",
    "            shutil.rmtree(os.path.join(root, d))\n",
    "    for root, dirs, files in os.walk(path_not_satisfying):\n",
    "        for f in files:\n",
    "            os.unlink(os.path.join(root, f))\n",
    "        for d in dirs:\n",
    "            shutil.rmtree(os.path.join(root, d))\n",
    "    i=0\n",
    "    for region in list(set(merged[:,4]))[1:]:\n",
    "        points = merged[merged[:,4]==region]\n",
    "        i = i+1\n",
    "        output_file = laspy.LasData(laspy.LasHeader(version=\"1.2\"))\n",
    "        output_file.X = (points[:,0]/scale[0]).astype(int)\n",
    "        output_file.Y = (points[:,1]/scale[1]).astype(int)\n",
    "        output_file.Z = (points[:,2]/scale[2]).astype(int)\n",
    "        output_file.x = points[:,0]\n",
    "        output_file.y = points[:,1]\n",
    "        output_file.z = points[:,2]\n",
    "        output_file.intensity = points[:,3]\n",
    "        if C[i-1]=='r':\n",
    "            path = '4-separated_trees/not_satisfying/'\n",
    "        else:\n",
    "            path = path_tree\n",
    "        if i<10:\n",
    "            output_file.write(f\"{path}tree0{i}.las\")\n",
    "        else:\n",
    "            output_file.write(f\"{path}tree{i}.las\")\n",
    "    \n",
    "    t = time.time() - start\n",
    "    \n",
    "    print(f'done，processed in {t} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94004b0c-2073-4c19-8b82-6ce916a19805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(start_location):\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import cv2\n",
    "    import laspy\n",
    "    from docx.shared import Cm\n",
    "    from docxtpl import DocxTemplate, InlineImage\n",
    "    from docx.enum.style import WD_STYLE_TYPE\n",
    "    from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "    from docxcompose.composer import Composer\n",
    "    from docx import Document as Document_compose\n",
    "    import datetime\n",
    "    import time\n",
    "    import os\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    QSM_path = '5-QSM_analysed_data/'\n",
    "    geo_path = QSM_path + 'geographic_data/'\n",
    "    topography = geo_path + 'slope_90M_n20e120.tif'\n",
    "    on_field_path = QSM_path + 'on_field_data/'\n",
    "    QSM_data_path = QSM_path + 'QSM/'\n",
    "    of_file = on_field_path +'on_field_data.xlsx'\n",
    "    QSM_file = QSM_data_path +'features.xlsx'\n",
    "  \n",
    "    subplots_path = geo_path + 'subplots_GPS_coordinates.xlsx'\n",
    "    Subplots = pd.read_excel(subplots_path)\n",
    "\n",
    "    LiDAR_file = '2023-05-17_08-14-07_10pct_ROI.las'\n",
    "    \n",
    "    cloud = laspy.read('2-intensity_filtered_point_cloud/'+LiDAR_file)\n",
    "    scale = cloud.header.scales\n",
    "    X , Y , Z = cloud.X*scale[0] , cloud.Y*scale[1] , cloud.Z*scale[2]\n",
    "    X , Y , Z = X.reshape(len(X),1) , Y.reshape(len(Y),1) , Z.reshape(len(Z),1)\n",
    "    cloud = np.hstack([X,Y,Z])\n",
    "    xmax = cloud[:,0].max()*180/((6.3781*10**6)*np.pi)\n",
    "    xmin = cloud[:,0].min()*180/((6.3781*10**6)*np.pi)\n",
    "    ymax = cloud[:,1].max()*180/((6.3781*10**6)*np.pi)\n",
    "    ymin = cloud[:,1].min()*180/((6.3781*10**6)*np.pi)\n",
    "    \n",
    "    surface = np.round((cloud[:,0].max()-cloud[:,0].min())*(cloud[:,1].max()-cloud[:,1].min()))\n",
    "\n",
    "    selection = []\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(Subplots)):\n",
    "        if Subplots.loc[i,'longitude']>start_location[0]+xmin:\n",
    "            if Subplots.loc[i,'longitude']<start_location[0]+xmax:\n",
    "                if Subplots.loc[i,'Latitude']>start_location[1]+ymin:\n",
    "                    if Subplots.loc[i,'Latitude']<start_location[1]+ymax:\n",
    "                        x.append(Subplots['longitude'][i])\n",
    "                        y.append(Subplots['Latitude'][i])\n",
    "                        selection.append(Subplots['TrapName'][i])\n",
    "\n",
    "    date = str(datetime.datetime.now().year)+'-'+str(datetime.datetime.now().month)+'-'+str(datetime.datetime.now().day)\n",
    "    sbplt = str(selection).replace(\"['\",'').replace(\"']\",'').replace(\",'\",'')\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(Subplots['longitude'],Subplots['Latitude'],c='r')\n",
    "    plt.scatter(x,y,c='g')\n",
    "    plt.scatter(start_location[0]+xmin,start_location[1]+ymin,marker='+',c='k',s=50)\n",
    "    plt.scatter(start_location[0]+xmax,start_location[1]+ymin,marker='+',c='k',s=50)\n",
    "    plt.scatter(start_location[0]+xmin,start_location[1]+ymax,marker='+',c='k',s=50)\n",
    "    plt.scatter(start_location[0]+xmax,start_location[1]+ymax,marker='+',c='k',s=50)\n",
    "    plt.xlabel('longitude (ºE)')\n",
    "    plt.ylabel('latitude (ºN)')\n",
    "    plt.title('FIGURE 1: LOCATION OF THE SCANNED AREA')\n",
    "    plt.savefig(f'{QSM_path}/generated_reports/AGB_report_{date}_{sbplt.replace(\", \",\"_\")}_fig1.png')\n",
    "    plt.show()\n",
    "\n",
    "    on_field_data = pd.read_excel(of_file)\n",
    "    QSM_data =  pd.read_excel(QSM_file)\n",
    "    on_field_data['Height'] = 0\n",
    "    for i in range(len(on_field_data)):\n",
    "        on_field_data.loc[i,'Height'] = np.cos(0.017*on_field_data.loc[i,'º Leaning'])*on_field_data.loc[i,'Living length (m)']\n",
    "    on_field_data = on_field_data.sort_values('plot').reset_index().drop(columns=['index'])\n",
    "    \n",
    "    subplots = []\n",
    "    for subplot in np.sort(list(set(on_field_data['plot']))):\n",
    "        if subplot in selection:\n",
    "            subplot_data = on_field_data.loc[on_field_data['plot']==f'{subplot}',:]\n",
    "            subplots.append(subplot_data)\n",
    "    species = []\n",
    "    real_DBH = np.array([])\n",
    "    real_H = np.array([]) \n",
    "    for subplot in subplots:\n",
    "        real_DBH = np.hstack([real_DBH,subplot['DBH '].values])\n",
    "        real_H = np.hstack([real_H,subplot['Height'].values])\n",
    "        for length in np.arange(len(list(set(subplot['Sp'])))):\n",
    "            a = list(set(subplot['Sp']))[length].replace('茄苳','bischofia javanica').replace('大葉桃花心木','swietenia macrophylla').replace('大葉欖仁','terminalia catappa').replace('白雞油','fraxinus griffithii').replace('黑板樹','alstonia scholaris').replace('印度紫檀','pterocarpus indicus').replace('台灣櫸','zelkova serrata')\n",
    "            if a not in species:\n",
    "                species.append(a)\n",
    "\n",
    "    nb_trees = len(real_DBH)\n",
    "    avDBH = real_DBH.mean()\n",
    "    stdDBH = real_DBH.std()\n",
    "    minDBH = real_DBH.min()\n",
    "    maxDBH = real_DBH.max()\n",
    "    avH = real_H.mean()\n",
    "    stdH = real_H.std()\n",
    "    minH = real_H.min()\n",
    "    maxH = real_H.max()\n",
    "    species = np.array(species)\n",
    "        \n",
    "    subplots = []\n",
    "    for subplot in np.sort(list(set(QSM_data['Subplot']))):\n",
    "        if subplot in selection:\n",
    "            subplot_data = QSM_data.loc[QSM_data['Subplot']==f'{subplot}',:]\n",
    "            subplots.append(subplot_data)\n",
    "    DBH , H , AGB , Brown_AGB = np.array([]) , np.array([]) , np.array([]) , np.array([])\n",
    "    for subplot in subplots:\n",
    "        DBH = np.hstack([DBH,subplot['DBH'].values])\n",
    "        H = np.hstack([H,subplot['height'].values])\n",
    "        AGB = np.hstack([AGB,subplot['model AGB'].values])\n",
    "        Brown_AGB = np.hstack([Brown_AGB,subplot[\"AGB Brown's equation\"].values])\n",
    "\n",
    "    detected_trees = len(os.listdir('4-separated_trees/not_satisfying')) + len(os.listdir('4-separated_trees/Trees'))\n",
    "    before_QSM_trees = len(os.listdir('4-separated_trees/Trees'))\n",
    "    final_trees = len(DBH)\n",
    "    model_avDBH = DBH.mean()\n",
    "    model_stdDBH = DBH.std()\n",
    "    model_minDBH = DBH.min()\n",
    "    model_maxDBH = DBH.max()\n",
    "    model_avH = H.mean()\n",
    "    model_stdH = H.std()\n",
    "    model_minH = H.min()\n",
    "    model_maxH = H.max()\n",
    "    model_avAGB = AGB.mean()\n",
    "    model_stdAGB = AGB.std()\n",
    "    model_minAGB = AGB.min()\n",
    "    model_maxAGB = AGB.max()\n",
    "    model_totalAGB = AGB.sum()\n",
    "    model_totalSAGB = model_totalAGB/surface\n",
    "    Brown_totalAGB = Brown_AGB.sum()\n",
    "    Brown_avAGB = Brown_AGB.mean()\n",
    "    Brown_stdAGB = Brown_AGB.std()\n",
    "    Brown_minAGB = Brown_AGB.min()\n",
    "    Brown_maxAGB = Brown_AGB.max()\n",
    "\n",
    "    plt.subplots(1,2,sharey=True,figsize=(12,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.scatter(DBH,AGB)\n",
    "    plt.xlabel('DBH (cm)')\n",
    "    plt.ylabel('estimated AGB (kg)')\n",
    "    plt.title('FIGURE 2.1: ESTIMATED AGB ACCORDING TO TREE DBH')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.scatter(H,AGB)\n",
    "    plt.xlabel('height (m)')\n",
    "    plt.title('FIGURE 2.2: ESTIMATED AGB ACCORDING TO TREE HEIGHT')\n",
    "    plt.savefig(f'{QSM_path}/generated_reports/AGB_report_{date}_{sbplt.replace(\", \",\"_\")}_fig2.png')\n",
    "    plt.show()\n",
    "\n",
    "    files = os.listdir('4-separated_trees/Trees/')\n",
    "    files = np.array(files)\n",
    "    files = files[files!='.DS_Store']\n",
    "    files_names = []\n",
    "    for file in files:\n",
    "        files_names.append(file[:-4])\n",
    "    trees = []\n",
    "    i = 0\n",
    "    for file_name in files_names:\n",
    "        i += 1\n",
    "        file = files[i-1]\n",
    "        CLOUD = laspy.read('4-separated_trees/Trees/'+file)\n",
    "        scale = CLOUD.header.scales\n",
    "        x , y , z = CLOUD.X*scale[0] , CLOUD.Y*scale[1] , CLOUD.Z*scale[2]\n",
    "        x , y , z = x.reshape(len(x),1) , y.reshape(len(y),1) , z.reshape(len(z),1)\n",
    "        intensity = CLOUD.intensity.reshape(len(x),1)\n",
    "        trees.append(np.hstack([x,y,z,intensity]))\n",
    "    plt.subplots(1,2,figsize=(12,6))\n",
    "    for tree in trees:\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.scatter(tree[:,0],tree[:,1],alpha=0.01,s=1)\n",
    "        plt.scatter(tree[:,:-1].mean(axis=0)[0],tree[:,:-1].mean(axis=0)[1],s=100,c='k',marker='+')\n",
    "        plt.xlabel('x axis (m)')\n",
    "        plt.ylabel('y axis (m)')\n",
    "        plt.title('FIGURE 3.1: SPATIAL REPARTITION, PLANAR VIEW')\n",
    "        plt.subplot(1,2,2)\n",
    "        id = tree[:,2].argmax()\n",
    "        plt.scatter(tree[id,0],tree[id,2],marker='^',c='k')\n",
    "        id1 = tree[:,2].argmin()\n",
    "        plt.scatter(tree[id1,0],tree[id1,2],marker='v',c='k')\n",
    "        plt.scatter(tree[:,0],tree[:,2],alpha=0.01,s=1)\n",
    "        plt.plot(np.array([tree[id,0],tree[id1,0]]),np.array([tree[id,2],tree[id1,2]]),'--',c='k')\n",
    "        plt.xlabel('x axis (m)')\n",
    "        plt.ylabel('height (m)')\n",
    "        plt.title('FIGURE 3.2: SPATIAL REPARTITION, SIDE VIEW')\n",
    "    plt.savefig(f'{QSM_path}/generated_reports/AGB_report_{date}_{sbplt.replace(\", \",\"_\")}_fig3.png')\n",
    "    plt.show()  \n",
    "\n",
    "    a , b = np.polyfit(real_H,real_DBH,1)\n",
    "    a1 , b1 = np.polyfit(H, DBH,1)\n",
    "    np.corrcoef(DBH,H)\n",
    "    np.corrcoef(real_DBH,real_H)[0,1]\n",
    "    R = np.round(np.corrcoef(DBH,H)[0,1],2)\n",
    "    R_real = np.round(np.corrcoef(real_DBH,real_H)[0,1],2)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.scatter(real_H,real_DBH,c='b')\n",
    "    plt.plot(np.linspace(real_H.min(),real_H.max(),len(real_H)),\n",
    "             a*np.linspace(real_H.min(),real_H.max(),len(real_H))+b,\n",
    "             '--',c='b',label=f'measured values, R = {R_real}')\n",
    "    plt.scatter(H,DBH,c='r')\n",
    "    plt.plot(np.linspace(H.min(),H.max(),len(H)),\n",
    "             a1*np.linspace(H.min(),H.max(),len(H))+b1,\n",
    "             '--',c='r',label=f'estimated values, R = {R}')\n",
    "    plt.xlabel('height (m)')\n",
    "    plt.ylabel(\"DBH (cm)\")\n",
    "    plt.title('FIGURE 4: DBH AND HEIGHT ESTIMATIONS OF OUR MODEL VERSUS ON-FIELD MEASUREMENTS')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{QSM_path}/generated_reports/AGB_report_{date}_{sbplt.replace(\", \",\"_\")}_fig4.png')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(np.linspace(1,max(Brown_maxAGB,model_maxAGB)),np.linspace(1,max(Brown_maxAGB,model_maxAGB)),'--',c='k')\n",
    "    plt.scatter(AGB,Brown_AGB,c='k')\n",
    "    plt.xlabel('estimated AGB (kg)')\n",
    "    plt.ylabel(\"AGB from Brown's equation (kg)\")\n",
    "    plt.title('FIGURE 5: DIFFERENCE BETWEEN THE ESTIMATED AGB AND BROWN’S EQUATION AGB FOR INDIVIDUAL TREES')\n",
    "    plt.savefig(f'{QSM_path}/generated_reports/AGB_report_{date}_{sbplt.replace(\", \",\"_\")}_fig5.png')\n",
    "    plt.show()\n",
    "\n",
    "    data = {\"date\": date,\n",
    "       \"location\": str(start_location).replace('[','').replace(']','').replace(',','ºE,')+'ºN',\n",
    "       \"subplot\": sbplt,\n",
    "       \"x\": np.round(cloud[:,0].max()-cloud[:,0].min()),\n",
    "       \"y\": np.round(cloud[:,1].max()-cloud[:,1].min()),\n",
    "       \"surface\": surface,\n",
    "       \"species\": str(species).replace(\"['\",'').replace(\"']\",'').replace(\",'\",','),\n",
    "       \"real_trees\": int(nb_trees),\n",
    "       \"model_trees\": int(detected_trees),\n",
    "       \"percentage\": np.round(100*detected_trees/nb_trees),\n",
    "       \"before_QSM_trees\": int(before_QSM_trees),\n",
    "       \"percentage1\": np.round(100*before_QSM_trees/nb_trees),\n",
    "       \"final_trees\": int(final_trees),\n",
    "       \"percentage2\": np.round(100*final_trees/nb_trees),\n",
    "       \"mDBH\": np.round(avDBH,1),\n",
    "       \"mh\": np.round(avH,1),\n",
    "       \"std_mDBH\": np.round(stdDBH,1),\n",
    "       \"std_mh\": np.round(stdH,1),\n",
    "       \"min_mDBH\": np.round(minDBH,1),\n",
    "       \"max_mDBH\": np.round(maxDBH,1),\n",
    "       \"min_mh\": np.round(minH,1),\n",
    "       \"max_mh\": np.round(maxH,1),\n",
    "       \"DBH\": np.round(model_avDBH,1),\n",
    "       \"h\": np.round(model_avH,1),\n",
    "       \"std_DBH\": np.round(model_stdDBH,1),\n",
    "       \"std_h\": np.round(model_stdH,1),\n",
    "       \"min_DBH\": np.round(model_minDBH,1),\n",
    "       \"max_DBH\": np.round(model_maxDBH,1),\n",
    "       \"min_h\": np.round(model_minH,1),\n",
    "       \"max_h\": np.round(model_maxH,1),\n",
    "       \"AGB\": np.round(model_totalAGB,1),\n",
    "       \"av_AGB\": np.round(model_avAGB,1),\n",
    "       \"std_AGB\": np.round(model_stdAGB,1),\n",
    "       \"min_AGB\": np.round(model_minAGB,1),\n",
    "       \"max_AGB\": np.round(model_maxAGB,1),\n",
    "       \"sAGB\": np.round(model_totalSAGB,1),\n",
    "       \"eqAGB\": np.round(Brown_totalAGB,1),\n",
    "       \"av_eqAGB\": np.round(Brown_avAGB,1),\n",
    "       \"std_eqAGB\": np.round(Brown_stdAGB,1),\n",
    "       \"min_eqAGB\": np.round(Brown_minAGB,1),\n",
    "       \"max_eqAGB\": np.round(Brown_maxAGB,1)}\n",
    "\n",
    "    template = DocxTemplate(QSM_path+'AGB_report_template.docx')\n",
    "    template.render(data)\n",
    "    template.add_picture(f'{QSM_path}/generated_reports/AGB_report_{date}_{sbplt.replace(\", \",\"_\")}_fig1.png',width=Cm(8))\n",
    "    last_paragraph = template.paragraphs[-1]\n",
    "    last_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    template.add_picture(f'{QSM_path}/generated_reports/AGB_report_{date}_{sbplt.replace(\", \",\"_\")}_fig2.png',width=Cm(16))\n",
    "    last_paragraph = template.paragraphs[-1]\n",
    "    last_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    template.add_picture(f'{QSM_path}/generated_reports/AGB_report_{date}_{sbplt.replace(\", \",\"_\")}_fig3.png',width=Cm(16))\n",
    "    last_paragraph = template.paragraphs[-1]\n",
    "    last_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    template.add_picture(f'{QSM_path}/generated_reports/AGB_report_{date}_{sbplt.replace(\", \",\"_\")}_fig4.png',width=Cm(10))\n",
    "    last_paragraph = template.paragraphs[-1]\n",
    "    last_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    template.add_picture(f'{QSM_path}/generated_reports/AGB_report_{date}_{sbplt.replace(\", \",\"_\")}_fig5.png',width=Cm(10))\n",
    "    last_paragraph = template.paragraphs[-1]\n",
    "    last_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    template.save(f'{QSM_path}/generated_reports/AGB_report_{date}_{sbplt.replace(\", \",\"_\")}.docx')\n",
    "    \n",
    "    t = time.time() - start\n",
    "\n",
    "    print(f'done，processed in {t} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a493d54-0ca7-459d-96de-528d15d50738",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1036319784.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    cell added to execute more easily the previous cell at launch\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "cell added to execute more easily the previous cell at launch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38b2273",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089fb4dd-c738-416c-b0b0-d9e3fc1071f9",
   "metadata": {},
   "source": [
    "## Parameters and executables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9ef1bf-6044-4f7f-8d01-3b7bc68c888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plots = 'y'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709de295-244a-4e6a-ad6b-d4a4ef065b7e",
   "metadata": {},
   "source": [
    "### 1: Intensity filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1067f2-932d-4f5f-abc4-67fe6a5f828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_file = '2023-05-17_08-14-07_10pct.las'\n",
    "nb_slices = 50\n",
    "ROI = 'auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a55fb0-0797-4d04-aabe-3008dab809b6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "intensity_filter(original_file,nb_slices,ROI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc79d271-bbf2-4c8e-a062-3c87b1fc9208",
   "metadata": {},
   "source": [
    "### 2 (Cloud Compare): classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb1598f-c3c0-4094-a4f9-df38a3febfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trees_file = 'T3S8.las'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1272f5de-9c3e-4b47-b49f-f1b257312aed",
   "metadata": {},
   "source": [
    "### 3: Co-registration of regions acquired with DBSCAN and 3D Forest, region growing and structure reinforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f80dde-9fb0-453f-8911-093b9d09067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROIzmin , ROIzmax = 0.5 , 1.0\n",
    "eps , min_samples = 0.1 , 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98ccfe1-469a-4387-b2ac-d8329b38420e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "DBSCAN_clustering(trees_file,ROIzmin,ROIzmax,eps,min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b870b296-faa0-452f-b023-4c186892d69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_to_merge = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd46112-66e5-4d34-9358-04ffe3a94f9d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "merging(dist_to_merge,ROIzmin,ROIzmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12280601-ca68-4302-9dd6-e205b3069c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_to_add_upper = 0.15\n",
    "dist_to_add_lower = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5323c949-81c4-4e7b-a8aa-ed6b72ab0334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region_growing(trees_file,dist_to_add_upper,dist_to_add_lower,ROIzmin,ROIzmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdccdbe3-78e3-48a4-bf9c-5d7c49d98a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_threshold = 0.05\n",
    "crown_threshold = 0.1\n",
    "height_for_crown_threshold = 3.5\n",
    "min_height = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de56a6cc-fd7e-40af-ab27-82d40aed9b1d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "nearest_nbr_reinforcement(original_file, general_threshold, crown_threshold, height_for_crown_threshold,min_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1072d1e5-68f0-4e7e-9c4a-71cc4b899b4b",
   "metadata": {},
   "source": [
    "### 4: QSM and result plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ee60a9-5184-4ac6-b3a5-aa26d60a4c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_location = [121.4951668,23.7890555]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f13824-0a8f-4d2a-88eb-d0815846fc8b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_results(start_location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
